
# ðŸš€ **Project: Real-Time Clickstream Ingestion (Kafka Mini Project)**

A beginnerâ€“friendly but industry-grade project to teach your interns **Kafka, real-time ingestion, event streaming, and basic analytics** for LMS user behavior.

This project fits perfectly into your AI-first LMS because **all student actions â†’ become live events â†’ feed analytics + personalization**.

---

# âœ… **1. Detailed SRS (Software Requirement Specification)**

## **1.1 Project Overview**

We track **every student click & activity** on the LMS platform in real time and ingest it into a Kafka cluster, store it in a Data Lake, and prepare it for analytics.

Clickstream includes:

* Page visits
* Button clicks
* Scroll events
* Search queries
* Video play/pause
* Quiz interactions
* Time spent ping events

This project lays the foundation for:

* Real-time dashboards
* Engagement analytics
* Recommendations
* Anomaly detection
* Future ML models

---

# **1.2 Functional Requirements**

### **FR-1: Collect Clickstream Events**

Events include:

* student_id
* event_name
* page_url
* chapter_id
* timestamp
* session_id
* device_type
* metadata (JSON)

### **FR-2: Frontend â†’ Kafka Producer**

LMS frontend sends events to:

```
POST /v1/track
```

Node.js backend pushes events â†’ Kafka topic:

```
topic: lms_clickstream
```

### **FR-3: Kafka Consumer**

A Python consumer listens in real time and pushes:

* raw events â†’ Data Lake
* processed events â†’ Data Warehouse

### **FR-4: Data Storage**

Use:

* **Raw Zone** â†’ JSON
* **Processed Zone** â†’ Parquet
* **Warehouse Schema** â†’ Analytics tables

### **FR-5: Real-Time Dashboard (Optional Mini Feature)**

Display:

* live user count
* live page activity
* top pages
* real-time student actions

### **FR-6: Monitoring & Retry**

* Failed events logged
* Retry mechanism
* Kafka lag monitoring

---

# **1.3 Non-Functional Requirements**

* **Scalability:** 50K events/minute
* **Latency:** < 100ms ingestion
* **Fault tolerance:** Kafka replication
* **Durability:** Events stored in Data Lake
* **Security:** Token-based API + masked student IDs
* **Compression:** Snappy/Gzip for Data Lake files

---

# **1.4 Stakeholders**

* Data Engineers
* Backend Engineers
* Data Analysts
* LMS Team
* ML Engineers

---

# ðŸ§± **2. Dataset Design**

Below are the tables after event processing.

---

## **Table 1 â€” clickstream_raw (JSON in Data Lake)**

| Field       | Type     |
| ----------- | -------- |
| event_id    | string   |
| student_id  | string   |
| event_name  | string   |
| page_url    | string   |
| chapter_id  | string   |
| session_id  | string   |
| device_type | string   |
| browser     | string   |
| timestamp   | datetime |
| metadata    | JSON     |

Example metadata:

```
{ "scroll_depth": 60, "duration": 120, "query": "polynomial" }
```

---

## **Table 2 â€” clickstream_processed (Parquet)**

| Field        | Description            |
| ------------ | ---------------------- |
| date         | Partition              |
| student_id   | Hashed/Pseudonymized   |
| session_id   | Session                |
| event_name   | click, view, scroll    |
| page_url     | URL                    |
| chapter_id   | LMS chapter            |
| duration     | time spent if provided |
| scroll_depth | %                      |
| timestamp    | event time             |

---

## **Table 3 â€” clickstream_analytics**

| Metric               | Meaning               |
| -------------------- | --------------------- |
| active_users         | AMU, DAU              |
| avg_session_duration | mean session time     |
| top_pages            | most visited          |
| scroll_behavior      | engagement metric     |
| click_frequency      | clicks/min user       |
| bounce_rate          | single-event sessions |

---

---

# ðŸ—ï¸ **3. Architecture Diagram**

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ LMS Frontend (React/JS)â”‚
                     â”‚  â†’ Sends click events  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ POST /track
                                   â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Node.js API Server   â”‚
                     â”‚Kafka Producer (events) â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                           Kafka Broker Cluster
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Topic: clickstream â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Kafka Consumer (Python/PySpark) â”‚
                    â”‚  â†’ Raw Zone (JSON)             â”‚
                    â”‚  â†’ Processed Zone (Parquet)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Data Lake (S3/MinIO)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Data Warehouse (BQ/Redshift) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Real-Time Dashboard (Superset) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ðŸ“ **4. Step-by-Step Tasks for Intern**

---

# **Phase 1 â€” Beginner**

### âœ” Task 1: Learn JSON clickstream data

âžœ Create sample event JSON
âžœ Understand fields & metadata

### âœ” Task 2: Set up Kafka locally (Docker)

Containers:

* kafka
* zookeeper
* kafka-ui

### âœ” Task 3: Create a Kafka topic

```
lms_clickstream
```

### âœ” Task 4: Build Kafka Producer

Small Node.js script to send a sample event every 2 seconds.

### âœ” Task 5: Build Kafka Consumer

Python consumer using:

```
confluent-kafka
```

---

# **Phase 2 â€” Intermediate**

### âœ” Task 6: Store raw events â†’ Data Lake

folder:

```
/data/raw/clickstream/YYYY/MM/DD/
```

### âœ” Task 7: Convert raw json â†’ processed parquet

using Pandas or PySpark

### âœ” Task 8: Build partition strategy

Partition by:

```
date
event_name
```

### âœ” Task 9: Create Data Warehouse tables

Using BigQuery / Snowflake / Redshift

---

# **Phase 3 â€” Advanced**

### âœ” Task 10: Real-time consumer with PySpark

Process events at scale.

### âœ” Task 11: Clickstream analytics aggregator

Compute:

* session duration
* scroll depth distribution
* most visited pages

### âœ” Task 12: Build real-time dashboard

Use:

* Apache Superset
* Metabase
* Grafana
* PowerBI

### âœ” Task 13: Create real-time API

```
GET /analytics/live-users
GET /analytics/top-pages
```

---

# **Phase 4 â€” Pro**

### âœ” Task 14: Add event schema validation

Using:

* JSON Schema
* Kafka Schema Registry

### âœ” Task 15: Event enrichment pipeline

Add:

* geo-location
* device fingerprint
* session reconstruction

### âœ” Task 16: Build anomaly detection

Detect:

* unusual traffic
* bot-like behavior
* sudden spikes

### âœ” Task 17: Build ML-ready feature dataset

* click frequency
* scroll depth
* learning patterns

### âœ” Task 18: Add monitoring

* Kafka lag
* Broken events
* Data Lake ingestion failures

---

# ðŸ“ **5. GitHub Folder Structure**

```
real-time-clickstream-ingestion/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ click-producer.js
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api-server.js
â”‚   â”œâ”€â”€ kafka-producer.js
â”‚
â”œâ”€â”€ consumer/
â”‚   â”œâ”€â”€ consumer.py
â”‚   â”œâ”€â”€ process_events.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ pyspark/
â”‚   â””â”€â”€ streaming_job.py
â”‚
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ schema/
â”‚       â”œâ”€â”€ clickstream_processed.sql
â”‚       â”œâ”€â”€ clickstream_analytics.sql
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ superset/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ clickstream_etl_dag.py
â”‚
â””â”€â”€ docs/
    â””â”€â”€ SRS.md
```

---

# ðŸŽ“ **6. Beginner â†’ Pro Curriculum (Intern Path)**

---

## **Module 1 â€” Beginner**

* Kafka basics
* Producer/Consumer concepts
* JSON event schema
* Basic Python

---

## **Module 2 â€” Intermediate**

* Kafka + Node.js producer
* Kafka + Python consumer
* Data Lake partitioning
* Parquet files
* ETL pipelines

---

## **Module 3 â€” Advanced**

* PySpark streaming
* Real-time processing
* Metrics aggregation
* Building dashboards

---

## **Module 4 â€” Pro**

* Schema Registry
* Event validation
* Sessionization
* ML-ready pipelines
* Monitoring & observability
